{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_ml_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNrOAfsQk_mE"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho-9sp_ilQym"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FUxCXaAlZVC"
      },
      "source": [
        "#jobdescription is tokenized into description\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "df = pd.read_csv(\"/content/sample_data/dice_com-job_us_sample.csv\")\n",
        "\n",
        "\n",
        "def identify_tokens(row):\n",
        "    jobdescription = row['jobdescription']\n",
        "    tokens = nltk.word_tokenize(jobdescription)\n",
        "    # taken only words (not punctuation)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words\n",
        "\n",
        "df['description'] = df.apply(identify_tokens, axis=1)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKkmIt93lb9c"
      },
      "source": [
        "# stopwords are removed from description and stored into new column named meaningful\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words(\"english\"))                  \n",
        "\n",
        "def remove_stops(row):\n",
        "    my_list = row['description']\n",
        "    meaningful_words = [w for w in my_list if not w in stops]\n",
        "    return (meaningful_words)\n",
        "\n",
        "df['meaningful'] = df.apply(remove_stops, axis=1)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqzs6Vanlmu7"
      },
      "source": [
        "#meaningful is again detokenized into meaninful string\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "df['meaningfulstring']=df['meaningful'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))\n",
        "\n",
        "df['meaningfulstring']=df['meaningfulstring'].str.lower()\n",
        "df['skills']=df['skills'].str.lower()\n",
        "df['jobtitle']=df['jobtitle'].str.lower()\n",
        "df['skills']=df['skills'].fillna(' ')\n",
        "\n",
        "# see below and see job description strings are removed from skills\n",
        "for i in df.index:\n",
        "  if(df['skills'].iloc[i]==\"see below\" or df['skills'].iloc[i]==\"(see job description)\" ):\n",
        "    df['skills'].iloc[i]=\" \"\n",
        "\n",
        "#skills and meaningfulstring is combined into one column named combined    \n",
        "def parameter(row):    \n",
        "    return row['skills']+\" \"+row['meaningfulstring']\n",
        "\n",
        "df['combined']=df.apply(parameter,axis=1)    \n",
        "df['combined'].head(3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3jP9-E7lgTg"
      },
      "source": [
        "#job id column is inserted to identify each tuple in the next step\n",
        "df.insert(0,'Job id',range(1,1+len(df)))\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oCwCOMIl5gl"
      },
      "source": [
        "#cosine simmilarity is applied\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cv = CountVectorizer()\n",
        "count_matrix = cv.fit_transform(df['combined'])\n",
        "cosine_sim = cosine_similarity(count_matrix)\n",
        "\n",
        "def get_index_from_job_type(title):\n",
        "    return df[df['jobtitle']==title]['Job id'].values[0]\n",
        "\n",
        "jobs_user_likes =\"java architect - denver, co - fulltime\"\n",
        "Job_id = get_index_from_job_type(jobs_user_likes)\n",
        "similar_jobs = list(enumerate(cosine_sim[Job_id]))\n",
        "sorted_similar_jobs = sorted(similar_jobs,key=lambda x:x[1],reverse=True)[1:]\n",
        "\n",
        "\n",
        "i=0\n",
        "print(\"Top 10 similar jobs to \"+jobs_user_likes+\" are:\\n\")\n",
        "for element in sorted_similar_jobs:\n",
        "    print(df.iloc[(element[0])])\n",
        "    i=i+1\n",
        "    if i>10:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}